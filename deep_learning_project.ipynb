{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/C cube dataset.csv')\n",
        "\n",
        "# Take first 2 columns: 'Question' and 'Answer'\n",
        "questions = data['Question'].astype(str).tolist()\n",
        "answers = data['Answer'].astype(str).tolist()\n",
        "\n",
        "# Add start and end tokens to answers\n",
        "answers = [\"<start> \" + a + \" <end>\" for a in answers]\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9?.!,]+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "questions = [clean_text(q) for q in questions]\n",
        "answers = [clean_text(a) for a in answers]\n",
        "# Tokenize questions\n",
        "tokenizer_q = Tokenizer(filters='')\n",
        "tokenizer_q.fit_on_texts(questions)\n",
        "question_seq = tokenizer_q.texts_to_sequences(questions)\n",
        "question_padded = pad_sequences(question_seq, padding='post')\n",
        "\n",
        "# Tokenize answers\n",
        "tokenizer_a = Tokenizer(filters='')\n",
        "tokenizer_a.fit_on_texts(answers)\n",
        "answer_seq = tokenizer_a.texts_to_sequences(answers)\n",
        "answer_padded = pad_sequences(answer_seq, padding='post')\n",
        "# Save vocabulary sizes\n",
        "VOCAB_SIZE_Q = len(tokenizer_q.word_index) + 1\n",
        "VOCAB_SIZE_A = len(tokenizer_a.word_index) + 1\n",
        "print(\"Questions padded shape:\", question_padded.shape)\n",
        "print(\"Answers padded shape:\", answer_padded.shape)\n",
        "print(\"Vocab size (Questions):\", VOCAB_SIZE_Q)\n",
        "print(\"Vocab size (Answers):\", VOCAB_SIZE_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCMceqKLVAX7",
        "outputId": "17bbd501-31ab-4717-8d1f-0eeb45e11d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions padded shape: (184, 10)\n",
            "Answers padded shape: (184, 21)\n",
            "Vocab size (Questions): 289\n",
            "Vocab size (Answers): 821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder, Decoder, and Seq2Seq (with GRU) â€” TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = GRU(enc_units,\n",
        "                   return_sequences=True,\n",
        "                   return_state=True,\n",
        "                   recurrent_initializer='glorot_uniform')\n",
        "    def call(self, x):\n",
        "      x = self.embedding(x)\n",
        "      output, state = self.gru(x)\n",
        "      return output, state\n",
        "class Decoder(tf.keras.Model):\n",
        "   def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "     super(Decoder, self).__init__()\n",
        "     self.dec_units = dec_units\n",
        "     self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "     self.gru = GRU(dec_units,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True,\n",
        "                    recurrent_initializer='glorot_uniform')\n",
        "     self.fc = Dense(vocab_size)\n",
        "   def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    x = self.fc(output)\n",
        "    return x, state\n",
        "\n",
        "VOCAB_SIZE = 10000  # change as per tokenizer\n",
        "EMBEDDING_DIM = 256\n",
        "UNITS = 512\n",
        "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
        "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vjyVPsBqZJGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop (Teacher Forcing)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the loss and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        "    )\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Loss function\n",
        "def loss_function(real, pred):\n",
        "   mask = tf.math.not_equal(real, 0)  # Ignore padding (0)\n",
        "   loss_ = loss_object(real, pred)\n",
        "   mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "   loss_ *= mask\n",
        "   return tf.reduce_mean(loss_)\n",
        "\n",
        "                        # Training step\n",
        "\n",
        "def train_step(inp, targ, encoder, decoder, batch_size, targ_lang_tokenizer):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "         enc_output, enc_hidden = encoder(inp)\n",
        "         dec_hidden = enc_hidden\n",
        "\n",
        "                                                        # <start> token assumed to be at index 1\n",
        "         dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "\n",
        "         for t in range(1, targ.shape[1]):\n",
        "               predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "               loss += loss_function(targ[:, t], predictions[:, 0, :])\n",
        "               dec_input = tf.expand_dims(targ[:, t], 1)  # Teacher forcing\n",
        "\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "FlJKYEJBR2Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad the sequences to ensure equal length\n",
        "input_tensor = pad_sequences(input_sequences, padding='post')\n",
        "target_tensor = pad_sequences(target_sequences, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "YoU2UQZDT9nr",
        "outputId": "9f49208b-3951-48ef-e1c4-ef4a8e2d357a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'input_sequences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-92b814e77ef2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Pad the sequences to ensure equal length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_sequences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = list(data['Question'])\n",
        "answers = list(data['Answer'])\n",
        "\n",
        "# Add <start> and <end> tokens to answers\n",
        "answers = ['<start> ' + ans + ' <end>' for ans in answers]\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenizer for questions\n",
        "input_tokenizer = Tokenizer(filters='')\n",
        "input_tokenizer.fit_on_texts(questions)\n",
        "input_sequences = input_tokenizer.texts_to_sequences(questions)\n",
        "\n",
        "# Tokenizer for answers\n",
        "target_tokenizer = Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(answers)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(answers)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "input_tensor = pad_sequences(input_sequences, padding='post')\n",
        "target_tensor = pad_sequences(target_sequences, padding='post')"
      ],
      "metadata": {
        "id": "MrINQ46KUL5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model hyperparameters\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Input and target vocab sizes\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "encoder = Encoder(input_vocab_size, embedding_dim, units)\n",
        "decoder = Decoder(target_vocab_size, embedding_dim, units)\n"
      ],
      "metadata": {
        "id": "h6tXIYYaUjQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume input_tensor and target_tensor are ready from preprocessing step\n",
        "# Define buffer size and batch size\n",
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "# Vocabulary sizes\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Initialize encoder and decoder\n",
        "encoder = Encoder(input_vocab_size, embedding_dim, units)\n",
        "decoder = Decoder(target_vocab_size, embedding_dim, units)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, encoder, decoder, BATCH_SIZE, target_tokenizer)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {total_loss / steps_per_epoch:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "23k0yD1LTNdh",
        "outputId": "2d797b5a-0a31-4cc5-c698-518d4a02e9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'encoder_3' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Model Encoder does not have a `call()` method implemented.''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'encoder_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling Encoder.call().\n\n\u001b[1mModel Encoder does not have a `call()` method implemented.\u001b[0m\n\nArguments received by Encoder.call():\n  â€¢ args=('tf.Tensor(shape=(64, 10), dtype=int32)',)\n  â€¢ kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b1906d0bdc40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-1fbbf23d2fef>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(inp, targ, encoder, decoder, batch_size, targ_lang_tokenizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m          \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m          \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         raise NotImplementedError(\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\"Model {self.__class__.__name__} does not have a `call()` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;34m\"method implemented.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Encoder.call().\n\n\u001b[1mModel Encoder does not have a `call()` method implemented.\u001b[0m\n\nArguments received by Encoder.call():\n  â€¢ args=('tf.Tensor(shape=(64, 10), dtype=int32)',)\n  â€¢ kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    }
  ]
}